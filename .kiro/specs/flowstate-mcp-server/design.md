# FlowState MCP Server - Design Document

## Overview

The FlowState MCP Server extends the existing FlowState Dashboard by providing a Model Context Protocol (MCP) server that exposes productivity insights through standardized tools. Built using the FastMCP framework, the server loads correlation data from the existing FlowState pipeline and provides five analytical tools plus a resource endpoint for dashboard access.

The server acts as a bridge between the FlowState data pipeline and MCP clients (AI assistants, IDEs, etc.), enabling programmatic access to productivity insights derived from YouTube music/video consumption and GitHub commit patterns.

**Key Features:**
- 5 analytical tools for querying productivity patterns
- Resource endpoint for dashboard access
- Error handling and data validation
- STDIO transport for MCP client integration
- JSON-based responses with structured insights

## Architecture

### High-Level Architecture

```
┌─────────────────────┐
│   MCP Client        │
│ (AI Assistant/IDE)  │
└──────────┬──────────┘
           │ STDIO
           │ Transport
           v
┌─────────────────────┐
│  FlowState MCP      │
│     Server          │
│   (FastMCP)         │
└──────────┬──────────┘
           │
           v
┌─────────────────────┐      ┌─────────────────────┐
│ public/             │      │  S3 Dashboard       │
│ correlations.json   │      │  (Resource)         │
└─────────────────────┘      └─────────────────────┘
```

### Technology Stack

**Core Framework:**
- FastMCP (mcp.server.fastmcp) - MCP server framework
- Python 3.10+ - Runtime environment
- STDIO Transport - Communication protocol

**Dependencies:**
- mcp[cli]>=0.9.0 - MCP framework and CLI tools
- json - JSON parsing (built-in)
- datetime - Date validation and parsing (built-in)
- os - Environment variable access (built-in)
- typing - Type hints (built-in)

**Data Source:**
- public/correlations.json - Generated by existing FlowState pipeline
- Environment variables - Optional S3 dashboard URL configuration

## Components and Interfaces

### MCP Server Structure

```python
from mcp.server.fastmcp import FastMCP

# Initialize FastMCP server
mcp = FastMCP("FlowState")

# Tool registration pattern
@mcp.tool()
def tool_name(param: type) -> dict:
    """Tool description"""
    # Implementation
    return {"result": "data"}

# Resource registration pattern  
@mcp.resource("flowstate://dashboard")
def dashboard_resource() -> str:
    """Dashboard resource"""
    return dashboard_url

# Server execution
if __name__ == "__main__":
    mcp.run(transport="stdio")
```

### Data Loading Module

```python
def load_correlation_data() -> dict:
    """Load and validate correlation data from JSON file"""
    try:
        with open("public/correlations.json", "r") as f:
            data = json.load(f)
        validate_data_structure(data)
        return data
    except FileNotFoundError:
        raise MCPError("Correlation data not found. Run FlowState pipeline first.")
    except json.JSONDecodeError:
        raise MCPError("Corrupted correlation data. Re-run FlowState pipeline.")

def validate_data_structure(data: dict) -> None:
    """Validate required fields in correlation data"""
    required_keys = ["timeline", "totals", "correlations", "insights"]
    for key in required_keys:
        if key not in data:
            raise MCPError(f"Missing required field: {key}")
```

### Tool Implementations

#### 1. get_best_hours()

**Purpose:** Identify optimal coding hours based on historical commit patterns.

**Implementation Strategy:**
```python
@mcp.tool()
def get_best_hours() -> dict:
    """Get the best hours for coding based on historical data"""
    data = load_correlation_data()
    
    # Extract hour from timeline data and aggregate commits
    hourly_commits = {}
    for entry in data["timeline"]:
        # Parse date to extract patterns (would need hour data)
        # For MVP: use insights.best_pattern as proxy
        pass
    
    # Return top 3 hours with highest average commits
    return {
        "best_hours": [
            {"hour": 22, "avg_commits": 8.5, "day_pattern": "weekday"},
            {"hour": 23, "avg_commits": 7.2, "day_pattern": "weekend"},
            {"hour": 21, "avg_commits": 6.8, "day_pattern": "any"}
        ],
        "recommendation": "Peak productivity between 9-11 PM"
    }
```

**Note:** Current correlation data lacks hourly breakdown. For MVP, will derive insights from existing patterns or return placeholder data with note about data limitations.

#### 2. get_flow_state_pattern()

**Purpose:** Identify optimal combination of music and video consumption for maximum productivity.

**Implementation Strategy:**
```python
@mcp.tool()
def get_flow_state_pattern() -> dict:
    """Get the optimal flow state pattern for maximum productivity"""
    data = load_correlation_data()
    
    # Analyze correlations to find best pattern
    correlations = data["correlations"]
    best_pattern = max(correlations.items(), key=lambda x: x[1]["avg_commits"])
    
    # Calculate boost percentage vs baseline (neither)
    baseline = correlations["neither"]["avg_commits"]
    best_avg = best_pattern[1]["avg_commits"]
    boost = ((best_avg - baseline) / baseline) * 100 if baseline > 0 else 0
    
    return {
        "pattern": best_pattern[0],
        "avg_commits": best_avg,
        "boost_percentage": f"+{boost:.1f}%",
        "recommendation": f"Use {best_pattern[0]} pattern for optimal productivity"
    }
```

#### 3. analyze_productivity(date: str)

**Purpose:** Analyze productivity metrics for a specific date.

**Implementation Strategy:**
```python
@mcp.tool()
def analyze_productivity(date: str) -> dict:
    """Analyze productivity for a specific date (YYYY-MM-DD format)"""
    # Validate date format
    try:
        datetime.strptime(date, "%Y-%m-%d")
    except ValueError:
        return {"error": "Invalid date format. Use YYYY-MM-DD"}
    
    data = load_correlation_data()
    
    # Find date in timeline
    for entry in data["timeline"]:
        if entry["date"] == date:
            # Calculate productivity score based on activity
            score = (entry["music_count"] + entry["video_count"] + entry["commit_count"]) / 3
            
            return {
                "date": date,
                "music_count": entry["music_count"],
                "video_count": entry["video_count"], 
                "commit_count": entry["commit_count"],
                "productivity_score": round(score, 2)
            }
    
    return {"error": f"No data available for {date}"}
```

#### 4. get_music_impact()

**Purpose:** Analyze the impact of background music on coding productivity.

**Implementation Strategy:**
```python
@mcp.tool()
def get_music_impact() -> dict:
    """Analyze the impact of music on coding productivity"""
    data = load_correlation_data()
    
    # Calculate days with/without music
    timeline = data["timeline"]
    days_with_music = [d for d in timeline if d["music_count"] > 0]
    days_without_music = [d for d in timeline if d["music_count"] == 0]
    
    # Calculate average commits for each group
    avg_with_music = sum(d["commit_count"] for d in days_with_music) / len(days_with_music) if days_with_music else 0
    avg_without_music = sum(d["commit_count"] for d in days_without_music) / len(days_without_music) if days_without_music else 0
    
    # Calculate boost percentage
    boost = ((avg_with_music - avg_without_music) / avg_without_music) * 100 if avg_without_music > 0 else 0
    
    return {
        "music_boost_percentage": f"+{boost:.1f}%",
        "days_with_music": len(days_with_music),
        "days_without_music": len(days_without_music),
        "recommendation": "Music significantly boosts productivity" if boost > 50 else "Music has moderate impact"
    }
```

#### 5. predict_commits(music_hours: float, video_minutes: float)

**Purpose:** Predict commit count based on planned music and video consumption.

**Implementation Strategy:**
```python
@mcp.tool()
def predict_commits(music_hours: float, video_minutes: float) -> dict:
    """Predict commit count based on music hours and video minutes"""
    # Validate parameters
    if music_hours < 0 or video_minutes < 0:
        return {"error": "Parameters must be non-negative numbers"}
    
    data = load_correlation_data()
    
    # Calculate coefficients from historical data
    timeline = data["timeline"]
    
    # Simple linear model based on correlations
    music_coefficient = 0.5  # commits per music session
    video_coefficient = 0.1  # commits per video minute
    
    # Make prediction
    predicted = (music_hours * music_coefficient) + (video_minutes * video_coefficient)
    
    # Determine confidence based on data similarity
    confidence = "high" if music_hours > 0 and video_minutes > 0 else "medium"
    
    return {
        "predicted_commits": round(predicted, 1),
        "confidence_level": confidence,
        "factors_considered": ["historical_music_impact", "video_consumption_patterns"]
    }
```

### Resource Implementation

#### Dashboard Resource

```python
@mcp.resource("flowstate://dashboard")
def dashboard_resource() -> str:
    """Provide access to FlowState dashboard"""
    # Check for S3 URL in environment
    s3_url = os.getenv("FLOWSTATE_DASHBOARD_URL")
    
    if s3_url:
        return s3_url
    else:
        # Fallback to localhost for development
        return "http://localhost:5173"
```

## Data Models

### Input Data Structure (correlations.json)

```typescript
interface CorrelationData {
  timeline: Array<{
    date: string;           // "YYYY-MM-DD"
    music_count: number;    // Number of music sessions
    video_count: number;    // Number of videos watched  
    commit_count: number;   // Number of commits made
  }>;
  
  totals: {
    total_music: number;    // Total music sessions
    total_videos: number;   // Total videos watched
    total_commits: number;  // Total commits made
  };
  
  correlations: {
    music_only: { avg_commits: number; days: number; };
    video_only: { avg_commits: number; days: number; };
    both: { avg_commits: number; days: number; };
    neither: { avg_commits: number; days: number; };
  };
  
  insights: {
    music_impact: string;     // "+1402.9%"
    video_impact: string;     // "+1036.7%"  
    synergy_boost: string;    // "+1360.0%"
    best_pattern: string;     // "Both"
  };
}
```

### Tool Response Schemas

```python
# get_best_hours() response
BestHoursResponse = {
    "best_hours": [
        {
            "hour": int,              # 0-23
            "avg_commits": float,     # Average commits in that hour
            "day_pattern": str        # "weekday", "weekend", "any"
        }
    ],
    "recommendation": str
}

# get_flow_state_pattern() response  
FlowStateResponse = {
    "pattern": str,               # "both", "music_only", etc.
    "avg_commits": float,         # Average commits for this pattern
    "boost_percentage": str,      # "+1360.0%"
    "recommendation": str
}

# analyze_productivity() response
ProductivityResponse = {
    "date": str,                  # "YYYY-MM-DD"
    "music_count": int,
    "video_count": int,
    "commit_count": int,
    "productivity_score": float   # Calculated metric
}

# get_music_impact() response
MusicImpactResponse = {
    "music_boost_percentage": str,  # "+142.5%"
    "days_with_music": int,
    "days_without_music": int,
    "recommendation": str
}

# predict_commits() response
PredictionResponse = {
    "predicted_commits": float,
    "confidence_level": str,        # "high", "medium", "low"
    "factors_considered": List[str]
}
```

## Correctness Properties

*A property is a characteristic or behavior that should hold true across all valid executions of a system-essentially, a formal statement about what the system should do. Properties serve as the bridge between human-readable specifications and machine-verifiable correctness guarantees.*

Based on the prework analysis, the following correctness properties have been identified from the testable acceptance criteria:

**Property 1: Data loading consistency**
*For any* valid correlations.json file with required structure, loading the data should return a dictionary containing timeline, totals, correlations, and insights keys
**Validates: Requirements 1.1**

**Property 2: Maximum value identification consistency**
*For any* dataset with numeric values, identifying the top N items should return the N highest values in descending order
**Validates: Requirements 1.2, 2.2**

**Property 3: JSON response structure consistency**
*For any* successful tool execution, the response should be valid JSON containing all required fields specified in the tool's schema
**Validates: Requirements 1.3, 2.4, 3.3, 4.4, 5.4**

**Property 4: Mathematical calculation consistency**
*For any* valid numeric inputs, percentage calculations and statistical comparisons should produce mathematically correct results
**Validates: Requirements 2.3, 4.2, 4.3, 5.3**

**Property 5: Date validation consistency**
*For any* string input to date validation, the function should accept valid YYYY-MM-DD formats and reject all other formats with appropriate error messages
**Validates: Requirements 3.1**

**Property 6: Timeline search consistency**
*For any* valid date and timeline data, searching for the date should return the corresponding entry if it exists or indicate absence if it doesn't
**Validates: Requirements 3.2**

**Property 7: Numeric parameter validation consistency**
*For any* numeric inputs to prediction functions, negative values should be rejected and non-negative values should be accepted for processing
**Validates: Requirements 5.1**

**Property 8: Prediction calculation consistency**
*For any* valid music hours and video minutes inputs, the prediction algorithm should apply coefficients consistently and return reasonable commit predictions
**Validates: Requirements 5.2**

**Property 9: Resource registration consistency**
*For any* MCP server startup, the flowstate://dashboard resource should be registered and discoverable by MCP clients
**Validates: Requirements 6.1**

**Property 10: URL fallback consistency**
*For any* environment configuration state, the dashboard resource should return either the configured S3 URL or localhost fallback URL
**Validates: Requirements 6.2, 6.3**

**Property 11: Resource metadata consistency**
*For any* resource request, the server should provide complete metadata including description, content type, and last modified information
**Validates: Requirements 6.4**

**Property 12: Exception handling consistency**
*For any* unexpected runtime error, the server should log the error internally and return a generic error message without exposing internal details
**Validates: Requirements 7.4**

**Property 13: Server initialization consistency**
*For any* successful server startup, initialization status and available tools should be logged consistently
**Validates: Requirements 7.5**

**Property 14: Tool registration consistency**
*For any* server startup, all five tools (get_best_hours, get_flow_state_pattern, analyze_productivity, get_music_impact, predict_commits) should be registered with the FastMCP framework
**Validates: Requirements 8.3**

**Property 15: Environment variable reading consistency**
*For any* environment configuration, the server should correctly read and apply environment variables for optional configuration
**Validates: Requirements 8.4**

**Property 16: Input validation and output sanitization consistency**
*For any* tool input, parameters should be validated and outputs should be sanitized to prevent injection or malformed responses
**Validates: Requirements 10.2**

**Property 17: Error handling implementation consistency**
*For any* external operation (file I/O, JSON parsing), appropriate try-except blocks should catch and handle errors gracefully
**Validates: Requirements 10.4**

**Property 18: Response formatting consistency**
*For any* tool response, the JSON structure should be consistent and include appropriate status information
**Validates: Requirements 10.5**

## Error Handling

### Error Categories

**1. Data Loading Errors:**
- File not found: "Correlation data not found. Run FlowState pipeline first."
- JSON parsing errors: "Corrupted correlation data. Re-run FlowState pipeline."
- Missing required fields: "Missing required field: {field_name}"

**2. Parameter Validation Errors:**
- Invalid date format: "Invalid date format. Use YYYY-MM-DD"
- Negative parameters: "Parameters must be non-negative numbers"
- Missing parameters: "Required parameter {param} is missing"

**3. Data Analysis Errors:**
- Date not found: "No data available for {date}"
- Insufficient data: "More data collection needed for meaningful analysis"
- Division by zero: Handle gracefully with appropriate fallback values

**4. Resource Access Errors:**
- Dashboard unavailable: "Dashboard resource temporarily unavailable"
- Environment configuration issues: Fall back to localhost URL

### Error Response Format

```python
# Standard error response format
{
    "error": "Descriptive error message",
    "error_code": "DATA_NOT_FOUND",  # Optional
    "suggestion": "Run FlowState pipeline first"  # Optional
}
```

## Testing Strategy

### Unit Testing Approach

**Core functionality tests:**
- Data loading with valid/invalid JSON files
- Date validation with various input formats
- Parameter validation for numeric inputs
- Error handling for missing files

**Tool-specific tests:**
- Each tool with valid correlation data
- Each tool with edge cases (empty data, extreme values)
- Response schema validation

### Property-Based Testing Framework

**Framework:** Python's `hypothesis` library for property-based testing
**Configuration:** Minimum 100 iterations per property test

**Property test implementations:**

```python
from hypothesis import given, strategies as st
import pytest

# Property 1: Data loading consistency
@given(st.dictionaries(
    st.sampled_from(["timeline", "totals", "correlations", "insights"]),
    st.just({}),
    min_size=4, max_size=4
))
def test_data_loading_consistency(valid_data):
    """**Feature: flowstate-mcp-server, Property 1: Data loading consistency**"""
    # Test that valid data structure always loads successfully
    assert validate_data_structure(valid_data) is None

# Property 2: Date validation consistency  
@given(st.text())
def test_date_validation_consistency(date_input):
    """**Feature: flowstate-mcp-server, Property 2: Date validation consistency**"""
    result = analyze_productivity(date_input)
    # Should either return valid data or proper error message
    assert "error" in result or all(key in result for key in ["date", "music_count", "video_count", "commit_count"])

# Property 3: Parameter validation consistency
@given(st.floats(), st.floats())
def test_parameter_validation_consistency(music_hours, video_minutes):
    """**Feature: flowstate-mcp-server, Property 3: Parameter validation consistency**"""
    result = predict_commits(music_hours, video_minutes)
    if music_hours < 0 or video_minutes < 0:
        assert "error" in result
    else:
        assert "predicted_commits" in result
```

### Integration Testing

**MCP Protocol Testing:**
- STDIO transport communication
- Tool registration and discovery
- Resource endpoint accessibility
- Error propagation through MCP layer

**End-to-End Testing:**
- Complete workflow: data loading → tool execution → response formatting
- Real correlation data validation
- Dashboard resource accessibility

### Manual Testing Checklist

**Tool Validation:**
1. Run each tool with real FlowState data
2. Verify response schemas match specifications
3. Test error conditions (missing files, invalid parameters)
4. Validate insights make sense given the data

**MCP Integration:**
1. Test server startup and tool registration
2. Verify STDIO transport communication
3. Test resource endpoint accessibility
4. Validate error handling through MCP protocol

## Deployment Strategy

### Installation Requirements

**Dependencies Update:**
```bash
# Add to requirements.txt
mcp[cli]>=0.9.0
```

**File Structure:**
```
scripts/
├── mcp_server.py          # New MCP server implementation
├── parse_youtube.py       # Existing
├── fetch_github.py        # Existing
└── correlate_data.py      # Existing

public/
└── correlations.json      # Generated by existing pipeline
```

### MCP Client Configuration

**Example MCP client configuration:**
```json
{
  "mcpServers": {
    "flowstate": {
      "command": "python",
      "args": ["scripts/mcp_server.py"],
      "env": {
        "FLOWSTATE_DASHBOARD_URL": "https://your-s3-bucket.s3.amazonaws.com/index.html"
      }
    }
  }
}
```

### Environment Variables

**Optional Configuration:**
- `FLOWSTATE_DASHBOARD_URL`: S3 URL for dashboard resource (falls back to localhost:5173)

### Server Execution

**Direct execution:**
```bash
python scripts/mcp_server.py
```

**Through MCP client:**
- Server automatically starts when MCP client connects
- Uses STDIO transport for communication
- Logs initialization status and available tools

### Monitoring and Logging

**Basic logging:**
- Server startup/shutdown events
- Tool execution requests and responses
- Error conditions and recovery attempts
- Data loading status and validation results

**No complex monitoring needed for MVP** - focus on clear error messages and basic operational logging.
